{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import bs4\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import DuplicateKeyError, CollectionInvalid\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "# Define the MongoDB database and table\n",
    "db_cilent = MongoClient()\n",
    "db = db_cilent['nyt_dump']\n",
    "table = db['articles']\n",
    "\n",
    "# Query the NYT API once\n",
    "def single_query(link, payload):\n",
    "    response = requests.get(link, params=payload)\n",
    "    if response.status_code != 200:\n",
    "        print 'WARNING', response.status_code\n",
    "    else:\n",
    "        return response.json()\n",
    "\n",
    "# Determine if the results are more than 100 pages\n",
    "def more_than_100_pages(total_page):\n",
    "    if total_page > 100:\n",
    "        pages_left = min(total_page - 100, 100)\n",
    "        return 100, pages_left, True\n",
    "    else:\n",
    "        return total_page, 0, False\n",
    "\n",
    "# Looping through the pages give the number of pages\n",
    "def loop_through_pages(total_pages, link, payload, table):\n",
    "    for i in range(total_pages):\n",
    "        if i % 50 == 0:\n",
    "            print ' || Page ||', i\n",
    "        payload['page'] = str(i)\n",
    "        content = single_query(link, payload)\n",
    "        meta_lst = content['response']['docs']\n",
    "\n",
    "        for meta in meta_lst:\n",
    "            try:\n",
    "                table.insert(meta)\n",
    "            except DuplicateKeyError:\n",
    "                print 'DUPS!'\n",
    "\n",
    "\n",
    "# Scrape the meta data (link to article and put it into Mongo)\n",
    "def scrape_meta(days=1):\n",
    "\n",
    "    # The basic parameters for the NYT API\n",
    "    # Url for NYT dev api\n",
    "    link = 'http://api.nytimes.com/svc/search/v2/articlesearch.json'\n",
    "    payload = {'api-key': '15504a1ea80590cff1e8aceed018cd55:10:72330163','fq': 'section_name:(\"World\" \"Travel\" \"Science\")'}\n",
    "\n",
    "    today = dt.datetime(2015, 6, 16)\n",
    "    for day in range(days):\n",
    "        payload['end_date'] = str(today).replace('-','').split()[0]\n",
    "        yesterday = today - dt.timedelta(hours=24)\n",
    "        payload['begin_date'] = str(yesterday).replace('-','').split()[0]\n",
    "        print 'Scraping period: %s - %s ' % (str(yesterday), str(today))\n",
    "\n",
    "        #print payload\n",
    "\n",
    "        \n",
    "        content = single_query(link, payload)\n",
    "        hits = content['response']['meta']['hits']\n",
    "        total_pages = min((hits / 10) + 1, 100)\n",
    "        print 'HITS', hits\n",
    "\n",
    "        today -= dt.timedelta(days=2)\n",
    "\n",
    "        loop_through_pages(total_pages, link, payload, table)\n",
    "\n",
    "# Get all the links, visit the page and scrape the content\n",
    "def get_articles(table):\n",
    "    links = db.table.find({'content_txt':{'$exists': False}},{'web_url':1})\n",
    "    counter = 0\n",
    "    for uid_link in links:\n",
    "        #time.sleep(1)\n",
    "        counter += 1\n",
    "        if counter % 100 == 0:\n",
    "            print 'Count: ', counter, ' '\n",
    "            print uid\n",
    "        uid = uid_link['_id']\n",
    "        link = uid_link['web_url']\n",
    "        print link\n",
    "        html = requests.get(link).text\n",
    "        soup = bs4.BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        article_content = [i.text for i in soup.select('.story-body-text.story-content')]\n",
    "        if not article_content:\n",
    "            article_content += [i.text for i in soup.select('.caption-text')]\n",
    "        if not article_content:\n",
    "            article_content += [i.text for i in soup.select('[itemprop=\"description\"]')]\n",
    "        if not article_content:\n",
    "            article_content += [i.text for i in soup.select('#nytDesignBody')]\n",
    "        else:\n",
    "            article_content += []\n",
    "\n",
    "        #table.update({'_id': uid}, {'$set': {'raw_html': html}})\n",
    "        table.update({'_id': uid}, {'$set': {'content_txt': article_content}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95375"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.find().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_articles(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
